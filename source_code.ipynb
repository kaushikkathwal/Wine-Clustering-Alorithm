{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of Class Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix:\n",
    "    # Constructor that loads data from a CSV file and optionally standardizes it\n",
    "\n",
    "    def __init__(self, filename, standardize=True):\n",
    "        self.array_2d = self.load_from_csv(filename)  # Load data into a 2D array\n",
    "        if standardize and self.array_2d is not None:\n",
    "            self.standardize()  # Standardize the data if needed\n",
    "\n",
    "    # Load CSV data and convert it into a NumPy array\n",
    "\n",
    "    def load_from_csv(self, filename):\n",
    "        try:\n",
    "            data = pd.read_csv(filename).to_numpy()  # Convert the data into a NumPy array\n",
    "            return data  # Return the loaded data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file: {e}\")  # Print an error message if loading fails\n",
    "            return None\n",
    "        \n",
    "    # Standardize each column of data by adjusting the values so they are more comparable\n",
    "\n",
    "    def standardize(self):\n",
    "        for j in range(self.array_2d.shape[1]):  # Go through each column\n",
    "            column = self.array_2d[:, j]  # Get all the values in that column\n",
    "            avg = np.mean(column)  # Find the average (mean) of the column\n",
    "            max_val = np.max(column)  # Get the maximum value\n",
    "            min_val = np.min(column)  # Get the minimum value\n",
    "            self.array_2d[:, j] = (column - avg) / (max_val - min_val)  # Standardize the column values\n",
    "\n",
    "    # Calculate the Euclidean distance from one row to all rows in another matrix\n",
    "\n",
    "    def get_distance(self, other_matrix, row_index):\n",
    "        row = self.array_2d[row_index]  # Get the row for which we're calculating distances\n",
    "        distances = np.sqrt(np.sum((other_matrix.array_2d - row) ** 2, axis=1))  # Calculate distances\n",
    "        return distances.reshape(-1, 1)  # Return distances as a column\n",
    "\n",
    "    # Calculate the Euclidean distance but with weights applied to each feature\n",
    "\n",
    "    def get_weighted_distance(self, other_matrix, weights, row_index):\n",
    "        row = self.array_2d[row_index]  # Get the specific row to compare\n",
    "        weighted_diffs = (other_matrix.array_2d - row) * weights.array_2d  # Apply the weights\n",
    "        distances = np.sqrt(np.sum(weighted_diffs ** 2, axis=1))  # Calculate weighted distances\n",
    "        return distances.reshape(-1, 1)  # Return as a column of distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standalone Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each value appears in S (used for counting group sizes)\n",
    "\n",
    "def get_count_frequency(S):\n",
    "    unique, counts = np.unique(S, return_counts=True)  # Get unique values and their counts\n",
    "    return dict(zip(unique.flatten(), counts))  # Return as a dictionary of counts\n",
    "\n",
    "# Generate random weights that sum to 1 (used for clustering)\n",
    "\n",
    "def get_initial_weights(num_columns):\n",
    "    weights = np.random.rand(1, num_columns)  # Create random weights\n",
    "    return weights / np.sum(weights)  # Normalize them so they sum to 1\n",
    "\n",
    "# Find the centroids (average points) of the clusters\n",
    "\n",
    "def get_centroids(data, S, K):\n",
    "    centroids = np.zeros((K, data.array_2d.shape[1]))  # Initialize centroids\n",
    "    for k in range(K):\n",
    "        # Calculate the mean (centroid) for each cluster\n",
    "        centroids[k] = np.mean(data.array_2d[S.flatten() == (k + 1)], axis=0)\n",
    "    return centroids  # Return the centroids\n",
    "\n",
    "# Calculate how spread out (separation) points are within each cluster\n",
    "\n",
    "def get_separation_within(data, centroids, S, K):\n",
    "    separation = np.zeros((1, data.array_2d.shape[1]))  # Initialize separation array\n",
    "    for j in range(data.array_2d.shape[1]):  # Go through each column (feature)\n",
    "        for k in range(K):\n",
    "            # Find all the points in cluster k and calculate their squared distance from the centroid\n",
    "            cluster_points = data.array_2d[S.flatten() == (k + 1)]\n",
    "            separation[0, j] += np.sum((cluster_points[:, j] - centroids[k, j]) ** 2)\n",
    "    return separation  # Return the separation values\n",
    "\n",
    "# Calculate how far apart the centroids of different clusters are\n",
    "\n",
    "def get_separation_between(data, centroids, S, K):\n",
    "    separation = np.zeros((1, data.array_2d.shape[1]))  # Initialize separation array\n",
    "    counts = np.array([np.sum(S.flatten() == (k + 1)) for k in range(K)])  # Get the number of points in each cluster\n",
    "    for j in range(data.array_2d.shape[1]):  # Go through each column\n",
    "        for k in range(K):\n",
    "            # Calculate the separation between the cluster centroid and the global mean\n",
    "            separation[0, j] += counts[k] * (centroids[k, j] - np.mean(data.array_2d[:, j])) ** 2\n",
    "    return separation  # Return the separation values\n",
    "\n",
    "\n",
    "\n",
    "# Assign each data point to a group (cluster) based on the nearest centroid\n",
    "def get_groups(data, K, verbose=False):\n",
    "    if K < 2 or K >= data.array_2d.shape[0]:\n",
    "        raise ValueError(f\"K must be in the range [2, {data.array_2d.shape[0] - 1}]\")  # Ensure valid number of clusters\n",
    "\n",
    "    weights = get_initial_weights(data.array_2d.shape[1])  # Get initial weights for clustering\n",
    "    S = np.zeros((data.array_2d.shape[0], 1))  # Initialize group assignments\n",
    "\n",
    "    # Randomly select initial centroids from the data\n",
    "    initial_indices = np.random.choice(data.array_2d.shape[0], K, replace=False)\n",
    "    centroids = data.array_2d[initial_indices]  # Set initial centroids\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Initial centroids:\\n{centroids}\")  # Print centroids if verbose mode is on\n",
    "\n",
    "    while True:\n",
    "        # Assign each point to the closest centroid\n",
    "        for i in range(data.array_2d.shape[0]):\n",
    "            distances = np.sqrt(np.sum((centroids - data.array_2d[i]) ** 2, axis=1))  # Calculate distances to centroids\n",
    "            S[i] = np.argmin(distances) + 1  # Assign to the nearest centroid\n",
    "\n",
    "        new_centroids = get_centroids(data, S, K)  # Recalculate centroids\n",
    "        if np.array_equal(new_centroids, centroids):  # Stop if centroids don't change\n",
    "            break\n",
    "        centroids = new_centroids  # Update centroids for the next iteration\n",
    "        if verbose:\n",
    "            print(f\"Updated centroids:\\n{centroids}\")  # Print new centroids if verbose mode is on\n",
    "\n",
    "    return S  # Return the group assignments\n",
    "\n",
    "# Update the weights based on how spread out (separated) the clusters are\n",
    "\n",
    "def get_new_weights(data, centroids, old_weights, S, K):\n",
    "    a = get_separation_within(data, centroids, S, K)  # Get separation within clusters\n",
    "    b = get_separation_between(data, centroids, S, K)  # Get separation between clusters\n",
    "    new_weights = old_weights + b / a  # Adjust weights based on separations\n",
    "    return new_weights / np.sum(new_weights)  # Normalize the weights to sum to 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the clustering with different numbers of groups (K) and iterations\n",
    "\n",
    "def run_test(): \n",
    "    m = Matrix('Data.csv')  # Load data from 'Data.csv'\n",
    "    for k in range(2, 11):  # Test for K = 2 to 10 clusters\n",
    "        for i in range(20):  # Repeat clustering 20 times for each K\n",
    "            S = get_groups(m, k)  # Get the group assignments\n",
    "            print(str(k) + \" = \" + str(get_count_frequency(S)))  # Print the number of points in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
